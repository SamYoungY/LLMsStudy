# A Survey on Large Language Model based Autonomous Agents（基于LLM的自主智能体的综述）

论文链接：https://arxiv.org/abs/2308.11432

GitHub：https://github.com/Paitesanshi/LLM-Agent-Survey

## 1.论文背景

之前的研究往往只能在特定的环境中训练具有特定知识的智能体，这与人类的学习过程有很大的分歧，从而使智能体难以实现类似人类的决策。而LLM经过大量数据的训练后已经掌握大量基础知识，具有一定的类人智能，基于这种能力，越来越多研究使用LLM来构建自主代理来获得类似人类的决策推理能力。

## 2.论文解决问题

目前已经有不少使用LLM作为Agent的研究并且都有一定的效果，但这些研究大都是独立提出，目前对他们的整体总结比较少，因此作者对这一领域的一些前沿研究进行了一个较系统的总结。

![]()

## 3.论文提出观点

文章主要从代理的构建，应用以及评估三个方面总结了目前的工作。

1.代理构建：主要关注两方面，即应选择哪种架构来更好地使用LLM，以及给出设计的架构，如何使代理获得完成特定任务的能力。

① 代理架构构建：文章提出一个统一架构总结目前开发的一些模块，主要由Profile模块、Memory模块、Planning模块和Action模块组成。

![]()

Profile模块：假定代理的角色，交代对应问题背景。

(1) 人工设定：手写提示配置agent的角色。

(2) LLM生成：先将一些背景告诉LLM，由它生成几种角色再进行选择。

(3) 数据集对齐：从真实数据集获取对应角色的特征来配置agent。

Memory模块：存储从环境中得到的信息，帮助agent积累经验。

(1) 记忆结构：只有短期记忆（统一存储），短期记忆和长期记忆混合两种结构。

(2) 记忆格式：自然语言，Embeddings，数据库，结构化列表。

(3) 记忆操作：读取，写入，反思。

Planning模块：对问题进行分解，计划未来的行动。

(1) 无反馈计划：Single-Path，如CoT等，Muti-Path，如ToT等。

(2) 有反馈计划：采取行动后会接受到来自环境，人类，和模型的反馈。

Action模块：受上面三个模块的指导，直接与外界进行交互。在这个阶段，agent可能需要借助外部的API，数据库，外部模型等，结合LLM内部知识，对环境执行动作空间中的某些动作，动作会对环境产生影响，也会改变agent内部的记忆与知识。

②代理能力获取：获取特定于任务的能力和经验。

(1) 基于微调方法：使用人工标注数据，大模型生成数据等对模型参数进行微调，适用于开源LLM。

(2) 无微调方法：基于提示工程和机制工程（例如加入试错，众包，经验积累等机制），适用于开源和闭源LLM。

![]()

2.代理应用：在社会科学，自然科学，工程方面的应用。

![]()

3.代理评估：评估LLM自主代理的有效性，分为主观评估和客观评估。

主观评估：将Agent的执行结果给到人类去打分，但成本较高。

客观评估：使用可以随时间计算、比较和跟踪的量化指标来评估基于LLM的自主代理的能力。

## 4.目前存在挑战

1.对于在之前的训练资料中出现较少或较新的角色，LLM可能不能很好地扮演其角色（LLM内部缺少这方面的知识）。解决方案可能包括人为收集对应数据进行微调，或者设计定制的代理提示。

2.传统的LLM内部通常与正确的人类价值观保持一致，但当agent应用于真实环境时，应当能模拟各种人类的特征，即根据特定场景令agent与不同的人类价值观对齐。因此，一个有趣的方向是如何通过设计适当的提示策略来“重新调整”这些模型。

3.LLM的提示词缺乏鲁棒性，对prompt微小的更改也可能产生截然不同的结果。解决方案：试错后手动写prompt，使用LLM自己生成prompt。

4.幻觉，可能的解决方案是添加人类反馈。

5.由于LLM的超出普通人的广泛知识网络，agent在模拟真实世界人类行为时可能会使用到真实人类无法获取的知识来进行决策，因此需要约束LLM的用户未知知识的使用。

## 5.总结

这篇文章对目前使用LLM作为Agent的研究作了一个概要总结，个人认为比较重要的是本文提出一个统一架构总结了目前开发的一些模块，让人比较清楚地认识到一个基于LLM的agent的大体架构以及架构下各模块是如何组织工作的。并且后面也提出了一些目前的难题和以后的一些研究方向，有一定的参考意义。