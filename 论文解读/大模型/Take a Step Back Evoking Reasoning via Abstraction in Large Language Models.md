# Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models

论文链接: https://arxiv.org/abs/2310.06117

## 1.论文背景

受到人们在面对复杂任务时通常会退一步，抽象化思考问题的启发，本文提出了“Step-Back Prompting”策略。通过这种方式使模型能够在进行推理时以更加抽象和高层次的概念为基础，从而减少在中间推理步骤中犯错误的可能性，最终达到提高整个推理过程正确性的目的。

## 2.论文方法

作者提出“Step-Back Prompting"，简称后退提示（STP），它是让 LLM 自己抽象问题，得到更高维度概念和原理，再用这些知识推理并解决问题。这种思维模式类似于人类解决问题的方式，让大模型能够借鉴已有规律解决问题。

该方法包含两个步骤：

1.抽象：首先提示 LLM 提出一个关于更高层次概念或原则的通用问题，并检索与之相关的信息，而不是直接回答原始问题。

2.推理：在获取了关于高层次概念或原则的信息后，LLM 可以基于这些信息对原始问题进行推理。

![](https://github.com/Kayin211/LLMsStudy/blob/master/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/pic/STP.png)

后退策略可能会让LLM尝试识别问题的范围和上下文，有的问题后退的多一点，有的少一些。

## 3.实验分析

作者用 PaLM-2L 和 GPT-4 模型做了实验，发现这种 Prompt 技巧对推理任务（STEM、知识问答、多步推理）的性能表现提升显著（高达27%）。

![](https://github.com/Kayin211/LLMsStudy/blob/master/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/pic/STP_result.png)

评估：采用了一种基于 PaLM-2L 模型的评估方法，通过 few-shot 学习来判断模型的答案是否与目标答案等价。

作者还进行了错误分析，发现大部分应用后退推理时出现的错误，都是由于 LLMs 推理能力的内在局限性造成的，与新的 prompt 技术无关。
而抽象能力又是 LLMs 比较容易学会的，所以这为后退推理的进一步发展指明了方向。

实验还表明将 Step-Back Prompting 与 RAG 结合能在某些方面提升模型效果。后退提示（STP）和RAG相结合，利用后退提示获得的抽象问题，获得更多与最终答案需要的的上下文信息，然后，再将获得的上下文和原始问题一起提交给LLM，从而让LLM获得更好的回答质量。

## 4.其它

10.3的一个新的研究《Large Language Models as Analogical Reasoners》提出，通过类比推理提示（Analogical Prompting）可以让大模型自己生成相似问题做为例子，从而再根据例子步骤形成思维链来解决新问题，提升了问题的泛化性，大模型可以根据问题不同生成不同的例子。

而10.9提出的“Step-Back Prompting”，不是类比寻找相似示例，而是让大语言模型自己把问题抽象化，得到一个更高维度的概念或者原理，再把抽象出来的知识当作工具，推理并得出问题的答案。

RAG：通过检索的方式，将问题相关的背景知识作为上下文一并传给大模型，有效的提高模型的准确性以及减轻幻觉。

## 5.总结

本文提出的“Step-Back Prompting”在复杂推理任务中通过全局观的思维提升模型效果，当然这也局限于一些需要深度推理的任务，而在简单问答中则没有效果。

对于复杂推理任务，传统手段是将其分解成多个子任务并解决，是一对多的关系，而本文提出的方法中的抽象手段则与其相反，专注于使问题变得更加抽象和高层次，抽象问题通常具有通用性，是多对一的关系。

综合最近的其他几篇论文发现，要让 LLM 能得到高质量的问答，和原始问题相关的高质量上下文信息是非常重要的，最好是不需要借助外部工具，直接通过特定的方法激发 LLM 自己去生成高质量的上下文信息。