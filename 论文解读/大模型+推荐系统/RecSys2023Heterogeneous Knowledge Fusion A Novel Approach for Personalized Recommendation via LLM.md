# RecSys2023:Heterogeneous Knowledge Fusion: A Novel Approach for Personalized Recommendation via LLM

论文链接：https://arxiv.org/abs/2308.03333

## 1.论文背景

作者通过LLM来处理用户的异构数据，通过优化prompt来进行推荐，实验表明方法有效

## 2.论文提出的问题

作者基于中国美团外卖的用户行为数据进行分析，发现用户行为之间有许多有用的语义数据，而这些异构数据在传统推荐系统上没有得到充分的利用，同时大量的行为主体导致稀疏特征，会对传统推荐系统的高效建模提出了挑战。

## 3.论文的解决方法

如图所示论文采用了对用户行为异构信息融合和指令微调这两种方法来提高推荐效果

![](https://raw.githubusercontent.com/XingYu-Zhong/LLMsStudy/master/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/pic/HKFR_key.png)

用户行为异构信息融合在本文作者没有告诉我们具体实现的方法和逻辑，但是本文说在推荐的时候是从数据库中检索这个数据的，那么可以推断出，这个异构信息本质上就是原本用户的行为数据，进行了数据清理，预测等操作后的特征数据和一些原本的文本数据和在一起给llm作为prompt。

指令微调本质上就是准备好了训练数据，一个input一个output，如何对chatglm-6b用lora的方法进行微调。

## 4.论文的实验

![](https://raw.githubusercontent.com/XingYu-Zhong/LLMsStudy/master/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/pic/HKFR_Experimental.png)

实验数据对比主要包含几方面，一部分是像Caser这样的传统推荐系统模型，一类是BERT4Rec这种加载了预训练模型Bert的推荐系统，还有一部分是像P5和ChatGLM-6B这样的大语言模型。剩下的就是作者提出来的使用用户异构数据微调的模型，其中分为三个，第一个是没有加指令微调的模型，我们可以看到其效果和ChatGLM差不多，仅仅好上一点，但不如P5，第二个是有指令微调，但没有做用户异构信息融合的，其效果明显优于前者且比P5还要好一点，第三个就是既做了指令微调又做了用户异构信息融合的，效果明显优于其他方法。我们可以明显看出模型在微调后，就能有不错的提升，而加上用户异构信息融合的数据也可以得到一点提升，但主要还是微调的提升比较明显。

## 5.论文实际效果

作者这个方法在美团外卖实际业务上ABtest的效果，主要是在点击率和冷启动上有一些提升，在其他方面上并没有看到明显的提升效果。

## 6.论文总结

这篇文章在具体方法实现上讲的模糊不清，可能涉及公司等其他因素吧，但他们的实验在美团外卖这样中国大型的互联网公司里的真实数据上做的，总体上可以看出使用llm做推荐确实能给实际业务带来一些提升。所以这篇文章给我们带来的提示：在处理llm推荐系统时主要分为两块，一块是在推荐用户数据处理方面需要做预测，融合等一些处理方法，第二块是微调模型能给推荐系统带来不错的提升。